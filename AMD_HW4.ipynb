{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Group 10\n",
    "\n",
    "## Our team:\n",
    "\n",
    "- Giulia Scikibu Maravalli\n",
    "- Katsiaryna Zavadskaya\n",
    "- Sara Cordaro \n",
    "\n",
    "*Libraries*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk # to clean the file\n",
    "from nltk.stem import PorterStemmer # for stemming\n",
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Does basic house information reflect house's description?  \n",
    "In this assignment we will perform a clustering analysis of house announcements in Rome from [Immobiliare.it](https://www.immobiliare.it/).  \n",
    "\n",
    "### Scraping  & Datasets\n",
    "\n",
    "Retrieving at least 10k announcements from *Immobiliare.it*, we created our dataset. Let's see how!  \n",
    "\n",
    "In our **main function**, we connect to the website (with our function **html_soup**, where we used *Beautiful Soup* library) and we iterate through its pages (every page contain 25 announcements) until we have processed 10 thousand announcements. Every time we pick an announcement we perform the following operations:\n",
    "\n",
    "- check if all the information we need are present (i.e. price, local, bathroom, floor, surface), otherwise we skip this announcement, and go to the next. To take these information we used **take_info** function.  \n",
    "\n",
    "\n",
    "- if we find the information above, we take the link of this announcement, and verify if there is the description (if not the house has been removed and we can go over it as well). To take description we used **take_description** function.\n",
    "\n",
    "\n",
    "- after we took the description, we processed it as a string: remove escape, punctuation and italian stopwords(this operation is executed by **clean_text** function). At the end of this cleaning, we save the modified description inside a new file, called *desc_n.txt* (where *n* is the number of the announcement). (**desc_file** function will do this).\n",
    "\n",
    "\n",
    "- At this point, if the announcement has both the required fields and description, we save every field in a dedicated list (look at the chunck of global variables, we create a list for each field).  \n",
    "\n",
    "And finally, we have our dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "# seconds to wait, to prevent the website block\n",
    "t = .400\n",
    "\n",
    "# url main\n",
    "immobiliare_link = 'https://www.immobiliare.it'\n",
    "\n",
    "# total number of announcements\n",
    "N = 10000\n",
    "\n",
    "# list of announcement's links\n",
    "link = []\n",
    "# list of announcement's prices\n",
    "price = []\n",
    "# list of announcement's locals\n",
    "local = []\n",
    "# list of announcement's surfaces\n",
    "surface = []\n",
    "# list of announcement's bathrooms\n",
    "bathroom = []\n",
    "# list of announcement's floors\n",
    "floor = []\n",
    "# list of announcement's numbers\n",
    "number = []\n",
    "# list of number of words\n",
    "doc_num_words = []\n",
    "\n",
    "# number of announcements\n",
    "num_announcement = 1\n",
    "# page of announcements\n",
    "page = 1\n",
    "\n",
    "\n",
    "# dictionary (key: word, value: list of idx repeat for the number of time that the word appears)\n",
    "dic_words = defaultdict(list)\n",
    "\n",
    "# dictionary (key: word, value: tfidf for each announcement)\n",
    "tfidf_dic = defaultdict(list)\n",
    "\n",
    "# escape\n",
    "escape = r'\\n'\n",
    "# set punctuation\n",
    "set_punt = list(string.punctuation)\n",
    "# set stopwords\n",
    "stop_words = set(nltk.corpus.stopwords.words('italian'))\n",
    "# stemming\n",
    "ps = PorterStemmer()\n",
    "# main path\n",
    "main_path = 'C:/Users/giuli/Desktop/AMD_HW4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the description (to remove punctuation and /n)\n",
    "def clean_text(text):\n",
    "    text = text.replace(escape, ' ')\n",
    "    # split description in a list\n",
    "    list_txt = text.split(' ')\n",
    "    # list of clean words\n",
    "    new_words = []\n",
    "    # iterate each word of description\n",
    "    for word in list_txt:\n",
    "        if word.isalpha():\n",
    "            if word not in set_punt:\n",
    "                # take each char in the string\n",
    "                for c in word:\n",
    "                    # delete char if it is a punctuation\n",
    "                    if c in set_punt:\n",
    "                        word = word.replace(c,\"\")\n",
    "                # save the string only if it isn't a stop words\n",
    "                if word not in stop_words: \n",
    "                    # we save the word in lowercase - so it is easier work in this way\n",
    "                    word = word.lower()\n",
    "                    new_words.append(word)\n",
    "    # convert description in strings\n",
    "    new_desc = ' '.join(new_words)\n",
    "    return new_desc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_file(idx,text):\n",
    "    text = clean_text(text)\n",
    "    # create a new folder where we save cleaned file\n",
    "    if not os.path.exists(main_path):\n",
    "        os.mkdir(main_path)\n",
    "    # create a file\n",
    "    with open(main_path + \"/desc_\" +str(idx) + \".txt\", \"w\") as f:\n",
    "        # write description\n",
    "        f.write(text)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html document\n",
    "def html_soup(url):\n",
    "    # request the server the content of the web page and store the server’s response - html document\n",
    "    while True:\n",
    "        try:\n",
    "            response = get(url)\n",
    "            html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            return html_soup\n",
    "        except:\n",
    "            time.sleep(t)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take description of each valid announcement\n",
    "def take_description(url_home,num):  \n",
    "    try:\n",
    "        html_home = html_soup(url_home)\n",
    "        desc_home = html_home.find('div', class_ =\"col-xs-12 description-text text-compressed\").text\n",
    "        desc_file(num,desc_home)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_info(announcement_info):\n",
    "    # try if the informations are not NONE\n",
    "    try:\n",
    "        infos = []\n",
    "        # take price of the home\n",
    "        price_info = announcement_info.find('li', class_=\"lif__item\").text\n",
    "        # clean the price\n",
    "        price_info = price_info.strip()        \n",
    "        if price_info.count(\"€\") >= 2:\n",
    "            price_info = \"€ \" + ''.join(price_info.split(\"€ \", 2)[:2])\n",
    "        # verify if it is only one home or a group of home\n",
    "        if 'da' not in price_info:\n",
    "            infos.append(price_info)\n",
    "            # take the other informations\n",
    "            all_info = announcement_info.find_all('div', class_ = \"lif__data\")\n",
    "            # save the other informations\n",
    "            for info in all_info[:-1]:\n",
    "                infos.append(info.span.text)\n",
    "            # take the home's floor    \n",
    "            info_floor = all_info[-1].find('abbr')['title']\n",
    "            infos.append(info_floor)\n",
    "            return infos\n",
    "    # if there are some informations NONE    \n",
    "    except:\n",
    "        infos = []\n",
    "        return infos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6348.990574359894\n"
     ]
    }
   ],
   "source": [
    "#MAIN FUNCTION\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# we want to analyze the info's on https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag=1\n",
    "# to retrieve at least 10k announcements\n",
    "while num_announcement <= N:\n",
    "    # assign the address of the web page to a variable named url\n",
    "    url = immobiliare_link + '/vendita-case/roma/?criterio=rilevanza&pag=' + str(page)\n",
    "    html_announcements = html_soup(url)    \n",
    "    # exctract all informations for each announcement\n",
    "    announcements_info = html_announcements.find_all('ul', class_ =\"listing-features list-piped\")\n",
    "    # extract all the div containers that have a class attribute of listing-item_body--content - result set\n",
    "    announcements_link = html_announcements.find_all('div', attrs={'class':'listing-item_body--content'})\n",
    "    # iterate each list of home's informations\n",
    "    for index in range(len(announcements_info)):\n",
    "        # take information of announcement\n",
    "        infos = take_info(announcements_info[index])\n",
    "        # check if there are all informations\n",
    "        if infos != None and (len(infos) == 5):\n",
    "            # take the home's link\n",
    "            home_link = announcements_link[index].a['href']\n",
    "            if immobiliare_link not in home_link:\n",
    "                home_link = immobiliare_link + home_link\n",
    "            # take description link of a home and control if the home is valid\n",
    "            control = take_description(home_link, num_announcement)\n",
    "            # if the home has the description is valid\n",
    "            if control == True:\n",
    "                # save all informations of a valid home in the global variables\n",
    "                link.append(home_link)\n",
    "                price.append(infos[0])\n",
    "                local.append(infos[1])\n",
    "                surface.append(infos[2])\n",
    "                bathroom.append(infos[3])\n",
    "                floor.append(infos[4])\n",
    "                number.append(int(num_announcement))\n",
    "        num_announcement += 1\n",
    "    page += 1\n",
    "    \n",
    "end_time = time.time()\n",
    "print(end_time - start_time)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe 1\n",
    "df_1 = pd.DataFrame({'Price': price,\n",
    "                       'Local': local,\n",
    "                       'Surface': surface,\n",
    "                       'Bathroom': bathroom,\n",
    "                       'Floor': floor}, index = number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Local</th>\n",
       "      <th>Surface</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Floor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>€ 225.000</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>€ 400.000</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>€ 500.000</td>\n",
       "      <td>3</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>€ 574.000</td>\n",
       "      <td>4</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>€ 300.000</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Price Local Surface Bathroom Floor\n",
       "2  € 225.000    2       50       1      1\n",
       "3  € 400.000    3       60       1      3\n",
       "4  € 500.000    3       89       2      3\n",
       "5  € 574.000    4       89       2      5\n",
       "6  € 300.000    2       46       1      4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved the dataframe with all required fields to a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe 1 in a file csv\n",
    "df_1.to_csv(main_path+'/matrix_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to build from this dataframe a real matrix, we need to filter it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataframe to create matrix_1\n",
    "\n",
    "# prices clean\n",
    "price_clean = []\n",
    "for p in price:\n",
    "    p = p.replace('€ ','')\n",
    "    p = p.replace(escape,'')\n",
    "    p = p.replace('.','')\n",
    "    price_clean.append(p)\n",
    "    \n",
    "# locals clean \n",
    "local_clean = []\n",
    "for l in local:\n",
    "    l = l.replace('+','')\n",
    "    local_clean.append(int(l))\n",
    "\n",
    "# surfaces clean\n",
    "surface_clean = []\n",
    "for s in surface:\n",
    "    s = s.replace('.','')\n",
    "    surface_clean.append(int(s))\n",
    "\n",
    "# bathrooms clean\n",
    "bath_clean = []\n",
    "for b in bathroom:\n",
    "    b = b.replace('+','')\n",
    "    bath_clean.append(int(b))\n",
    "\n",
    "data_1 = pd.DataFrame({'Price': price_clean,\n",
    "                       'Local': local_clean,\n",
    "                       'Surface': surface_clean,\n",
    "                       'Bathroom': bath_clean,\n",
    "                       'Floor': floor})\n",
    "\n",
    "data_1 = data_1[data_1.Price.apply(lambda x: x.isdigit())]\n",
    "data_1 = data_1[data_1['Local'].apply(lambda x: (str(x)).isdigit())]\n",
    "data_1 = data_1[data_1['Surface'].apply(lambda x: str(x).isdigit())]\n",
    "data_1 = data_1[data_1['Bathroom'].apply(lambda x: (str(x)).isdigit())]\n",
    "data_1 = data_1[data_1['Floor'].apply(lambda x: (str(x)).isdigit())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Local</th>\n",
       "      <th>Surface</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Floor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225000</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>400000</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500000</td>\n",
       "      <td>3</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>574000</td>\n",
       "      <td>4</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300000</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price  Local  Surface  Bathroom Floor\n",
       "0  225000      2       50         1     1\n",
       "1  400000      3       60         1     3\n",
       "2  500000      3       89         2     3\n",
       "3  574000      4       89         2     5\n",
       "4  300000      2       46         1     4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrix 1\n",
    "matrix_1 = data_1.values\n",
    "matrix_1 = np.matrix(matrix_1).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the obtained matrix into a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the matrix 1 in a file txt\n",
    "np.savetxt('last_matrix_1.txt', matrix_1, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after we have built the first dataframe, we need to create the second one. In order to do so, we first iterate through all the *desc_n.txt* created before and for each of them, we stem all the words in the file and store the n. of words in *doc_num_words* list. At the end of iteration we create a new file, called *vocabulary.txt* where we save a dictionary containing as keys the word and as value a list that have the index of the announcement in which that word appeared (if the word appears muliple times in an announcement, that index will be repeated). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabularies\n",
    "# iterate the files\n",
    "for idx in number:\n",
    "    with open(main_path + \"/desc_\" +str(idx)+ \".txt\") as f:\n",
    "        # split the line according to '\\t'\n",
    "        line = f.read()\n",
    "        list_file = line.split(' ')\n",
    "        # save the word as key and the index of the file in list of values\n",
    "        for word in list_file:\n",
    "            # stemming\n",
    "            word = ps.stem(word)\n",
    "            dic_words[word].append(idx)\n",
    "        num_words = len(list_file)\n",
    "        doc_num_words.append(num_words)\n",
    "# save dictionary in a file\n",
    "with open(main_path +'/vocabulary.txt', 'w') as f:\n",
    "        f.write(json.dumps(dic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute *tf-Idf* and built the second matrix in the format requested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the tfIdf (announcement, word)\n",
    "def compute_tfIdf(word, idx , voc):\n",
    "    # tf is term frequences on a determinate description\n",
    "    tf =(voc[word].count(number[idx]))/(doc_num_words[idx])\n",
    "    # df document frequences\n",
    "    df = len(set(voc[word]))\n",
    "    # idf inverse document frequency\n",
    "    idf = math.log10(len(number)/df)\n",
    "    return(tf*idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and save vocabulary\n",
    "vocabulary = json.load(open(main_path +'/vocabulary.txt', 'rb'))\n",
    "list_words = sorted(list(vocabulary.keys()))\n",
    "for word in list_words:\n",
    "    for idx in range(len(number)):\n",
    "        tfidf = compute_tfIdf(word,idx,vocabulary)\n",
    "        tfidf_dic[word].append(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the result in a dataframe, and then save it as .csv and .txt (as a matrix) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe 2\n",
    "df_2 = pd.DataFrame(data = tfidf_dic, index = number)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>aabbiamo</th>\n",
       "      <th>ab</th>\n",
       "      <th>abacu</th>\n",
       "      <th>abamelek</th>\n",
       "      <th>abano</th>\n",
       "      <th>abb</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbaini</th>\n",
       "      <th>abbaino</th>\n",
       "      <th>...</th>\n",
       "      <th>élégant</th>\n",
       "      <th>énergétiqu</th>\n",
       "      <th>équipé</th>\n",
       "      <th>étage</th>\n",
       "      <th>état</th>\n",
       "      <th>étude</th>\n",
       "      <th>été</th>\n",
       "      <th>ìntegrata</th>\n",
       "      <th>último</th>\n",
       "      <th>über</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 15571 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     a  aabbiamo   ab  abacu  abamelek  abano  abb  abba  abbaini  abbaino  \\\n",
       "2  0.0       0.0  0.0    0.0       0.0    0.0  0.0   0.0      0.0      0.0   \n",
       "3  0.0       0.0  0.0    0.0       0.0    0.0  0.0   0.0      0.0      0.0   \n",
       "4  0.0       0.0  0.0    0.0       0.0    0.0  0.0   0.0      0.0      0.0   \n",
       "5  0.0       0.0  0.0    0.0       0.0    0.0  0.0   0.0      0.0      0.0   \n",
       "6  0.0       0.0  0.0    0.0       0.0    0.0  0.0   0.0      0.0      0.0   \n",
       "\n",
       "   ...   élégant  énergétiqu  équipé  étage  état  étude  été  ìntegrata  \\\n",
       "2  ...       0.0         0.0     0.0    0.0   0.0    0.0  0.0        0.0   \n",
       "3  ...       0.0         0.0     0.0    0.0   0.0    0.0  0.0        0.0   \n",
       "4  ...       0.0         0.0     0.0    0.0   0.0    0.0  0.0        0.0   \n",
       "5  ...       0.0         0.0     0.0    0.0   0.0    0.0  0.0        0.0   \n",
       "6  ...       0.0         0.0     0.0    0.0   0.0    0.0  0.0        0.0   \n",
       "\n",
       "   último  über  \n",
       "2     0.0   0.0  \n",
       "3     0.0   0.0  \n",
       "4     0.0   0.0  \n",
       "5     0.0   0.0  \n",
       "6     0.0   0.0  \n",
       "\n",
       "[5 rows x 15571 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe 2 in a file csv\n",
    "df_2.to_csv(main_path+'/matrix_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrix 2\n",
    "matrix_2 = df_2.values\n",
    "matrix_2 = np.matrix(matrix_2).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the matrix 2 in a file txt\n",
    "np.savetxt('last_matrix_2.txt', matrix_2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also save the list of announcement's number beacuse it will be useful in next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save list of announcement's number\n",
    "with open('list', 'wb') as fp:\n",
    "    pickle.dump(number, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Find the duplicates!\n",
    "Given [passwords2.txt](https://drive.google.com/file/d/1wTmOU-yqk4qdQYg42AquhzgpNGrRA96d/view) file as input, our goal is to find how many duplicates are present inside it. At first, we consider duplicates not only equal strings, but also strings that contain the same characters (i.e. 'AABA' = 'AAAB'), further our check is case sensitive, thus 'AaBa' $\\neq$ 'AAba', if you want to make our code unsensitive about lower/uppercase you just need to uncomment in the definition of our *hashCode()* function this line: *sorted_chars[i] = sorted_chars[i].lower()* and run it.\n",
    "\n",
    "We proceeded in this way:\n",
    "\n",
    "- iterate through every line in the file (each line is a string of 20 characters).\n",
    "\n",
    "- trasform each string in a list of strings (e.g. 'AABA' -> \\['A','A','B','A'\\]).\n",
    "\n",
    "- sort the list, in this way we made equal  words with same charcter in different order (now characters are in the same order).\n",
    "\n",
    "- then we iterate through every character, and with the function **convertToNumber** we convert the character to a number and then to have a very large numeber, we multiplied that integer to a high number: 17^esp. Then we sum the value of each character of that string. In this way we transform the string into a large number (take a look to **hashCode** function defined below).  \n",
    "Note: *esp* is the index of the character, right now this not particularly important since we sort the order of the characters (but in the second part of this exercize, when we do not sort them, it will allow us to differenciate bewtween word with same character in different order).\n",
    "\n",
    "- with **HashMap** function we did the modulus between our large number (converted string) and 100011723599, a large prime number. We decided to use this value because as large number it increases the number of possible results of the modulus and as prime number it allows us to diversify as much as possible the outputs.\n",
    "\n",
    "- we created a bitarray of length 100011723599, composed by zero (i.e. an array of 100011723599 zeros).\n",
    "\n",
    "- agan with *HashMap* we took the result of the modulus, go to that position in the bitarray, and change the zero to one, if in that position there is already a one, it means that another string had (after the process above) the same value (it is a duplicate!!!). If that is the case we add 1 to a counter, the variable *duplicates*. \n",
    "\n",
    "- at the end of iteration we check the value of *duplicates*, intuitively it will tell us the number of duplicates inside the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10049831\n"
     ]
    }
   ],
   "source": [
    "def convertToNumber(s):\n",
    "    return int.from_bytes(s.encode(), 'little')\n",
    "\n",
    "def hashCode(pw):\n",
    "    # list of chars in the password\n",
    "    chars = list(pw)\n",
    "    # doesn't matter the order of the password\n",
    "    sorted_chars= sorted(chars)\n",
    "    # new num associated to the pw\n",
    "    pw_num = 0\n",
    "    for i in range(20):\n",
    "        #uncomment line below if you do not want a case sensitive approach\n",
    "        #sorted_chars[i] = sorted_chars[i].lower()\n",
    "        w_encode = convertToNumber(sorted_chars[i])\n",
    "        exp = 19 - i\n",
    "        pw_num += w_encode * (17**exp)\n",
    "    return pw_num\n",
    "\n",
    "prime_n = 100011723599\n",
    "# initialize bitarry length prime_n \n",
    "bit_array = bitarray(prime_n)\n",
    "bit_array.setall(0)\n",
    "\n",
    "def hashMap(pw_num):\n",
    "    count = 0\n",
    "    index = abs(pw_num)%prime_n\n",
    "    if (bit_array[index] == 0):\n",
    "        bit_array[index] = 1\n",
    "    else:\n",
    "        count = 1\n",
    "    return count\n",
    "\n",
    "# iterate passwords in the file.txt\n",
    "duplicates = 0\n",
    "\n",
    "with open('passwords2.txt') as pw_file:\n",
    "    for pw in pw_file:\n",
    "        pw_num = hashCode(pw)\n",
    "        duplicates += hashMap(pw_num)\n",
    "\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a result very close to 10 milion (the real number of duplicate in the file). However we have some *false positives*, 49831. \n",
    "\n",
    "#### Now we do the same thing as before, but taking into account the order of the characters.  \n",
    "We use the same code of the previuos part, but this time we do not sort the list of character in the string, then word with same character in different position will remain different. As said in the note above, after tranforming the character to an integer we multiply it to 17^*esp*, *esp* is the position of the character inside the string, thanks to this different word with same character will have different numeric values. For the rest, the code remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5055169"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prime_n = 100011723599\n",
    "# initialize bitarry length prime_n \n",
    "bit_array = bitarray(prime_n)\n",
    "bit_array.setall(0)\n",
    "\n",
    "def hashCode(pw):\n",
    "    # list of chars in the password\n",
    "    chars = list(pw)\n",
    "    # new num associated to the pw\n",
    "    pw_num = 0\n",
    "    for i in range(20):\n",
    "        #uncomment line below if you do not want a case sensitive approach\n",
    "        #chars[i] = chars[i].lower()\n",
    "        w_encode = convertToNumber(chars[i])\n",
    "        exp = 19 - i\n",
    "        pw_num += w_encode * (17**exp)\n",
    "    return pw_num\n",
    "\n",
    "# iterate passwords in the file.txt\n",
    "duplicates1 = 0\n",
    "\n",
    "with open('passwords2.txt') as pw_file:\n",
    "    for pw in pw_file:\n",
    "        pw_num = hashCode(pw)\n",
    "        duplicates1 += hashMap(pw_num)\n",
    "\n",
    "duplicates1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be around 50 thousand duplicates, taking into account the order of characters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
